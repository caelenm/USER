{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vZGkYqoM8tq7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray\n",
        "!pip install optuna\n",
        "from ray import tune, air\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "from ray.air import session\n",
        "from ray.air.config import RunConfig, ScalingConfig\n",
        "from ray.tune import Tuner"
      ],
      "metadata": {
        "id": "lVDKt2Mhw-CT",
        "outputId": "c11b0087-fa7a-42fe-d727-3469819e446a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.34.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.7.4)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-guCYDAe9EBR"
      },
      "outputs": [],
      "source": [
        "#dataset\n",
        "unseen_emotion = \"angry\"\n",
        "folder = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "# where emotion word embeddings are stored\n",
        "fasttext_folder='/content/drive/MyDrive/emotion_vectors'\n",
        "#where to save the model\n",
        "model_save = 'advanced_embedding_mapper.pth'\n",
        "#where the pickle is saved\n",
        "pickle_path = '/content/drive/MyDrive/USER/fileTensorDict.pckl'\n",
        "seed = 420"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "afP114cg84E9"
      },
      "outputs": [],
      "source": [
        "def get_emotion_vector(filename):\n",
        "    parts = filename.split('-')\n",
        "    third_number = parts[2]\n",
        "\n",
        "    emotion_vector_label = None\n",
        "    if third_number == '05':\n",
        "        emotion_vector_label = 'angry'\n",
        "    elif third_number == '02':\n",
        "        emotion_vector_label = 'calm'\n",
        "    elif third_number == '07':\n",
        "        emotion_vector_label = 'disgust'\n",
        "    elif third_number == '06':\n",
        "        emotion_vector_label = 'fearful'\n",
        "    elif third_number == '03':\n",
        "        emotion_vector_label = 'happy'\n",
        "    elif third_number == '01':\n",
        "        emotion_vector_label = 'neutral'\n",
        "    elif third_number == '04':\n",
        "        emotion_vector_label = 'sad'\n",
        "    elif third_number == '08':\n",
        "        emotion_vector_label = 'surprised'\n",
        "\n",
        "    return emotion_vector_label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_emotion_vectors(folder):\n",
        "    emotion_vectors = {}\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            emotion_name = filename.split('.')[0]\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            with open(filepath, 'r') as file:\n",
        "                vector = [float(line.strip()) for line in file]\n",
        "                emotion_vectors[emotion_name] = torch.tensor(vector, dtype=torch.float32)\n",
        "    return emotion_vectors"
      ],
      "metadata": {
        "id": "keDBeSEvVfNo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lsbj_J2r9Fg8"
      },
      "outputs": [],
      "source": [
        "def addToDict(folder):\n",
        "  emo_dict = {}\n",
        "  for file in os.listdir(folder):\n",
        "      emo_dict[file] = get_emotion_vector(file)\n",
        "  str_emo_dict = str(emo_dict)\n",
        "  with open(\"vectors.txt\", \"a\") as vec:\n",
        "    vec.write(str_emo_dict)\n",
        "\n",
        "for x in os.listdir(folder):\n",
        "  addToDict(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "39EHaFxC9P50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e18112-c8b4-4765-d6cb-f547a85c4a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Aum2q6rN9X3m"
      },
      "outputs": [],
      "source": [
        "def get_vector_from_audio(path):\n",
        "  audio_input, sampling_rate = librosa.load(path, sr=16000)\n",
        "  inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "  hidden_states = outputs.last_hidden_state\n",
        "\n",
        "  vector_rep = torch.mean(hidden_states, dim=1)\n",
        "  return vector_rep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ydppCfjl9lgI"
      },
      "outputs": [],
      "source": [
        "def get_embeddings():\n",
        "  filename_vector_dict = {}\n",
        "  # path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "  for f in os.listdir(path):\n",
        "    file_path = os.path.join(path,f)\n",
        "    emov = get_vector_from_audio(file_path)\n",
        "    filename_vector_dict[f] = emov\n",
        "  return filename_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CKsVEpCo9qIm"
      },
      "outputs": [],
      "source": [
        "def load_fasttext_embedding(emotion_label, fasttext_folder):\n",
        "  filepath = os.path.join(fasttext_folder, f'{emotion_label}.txt')\n",
        "  if not os.path.exists(filepath):\n",
        "      raise FileNotFoundError(f\"Embedding file for {emotion_label} not found in {folder}\")\n",
        "\n",
        "  embedding = []\n",
        "  with open(filepath, 'r') as file:\n",
        "      for line in file:\n",
        "          embedding.append(float(line.strip()))\n",
        "\n",
        "  return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ILxRzWeT9uqc"
      },
      "outputs": [],
      "source": [
        "def map_fasttext_to_wav2vec(wav2vec_dict, fasttext_folder='emotion_vectors'):\n",
        "    fasttext_vector_dict = {}\n",
        "    for filename, wav2vec_embedding in wav2vec_dict.items():\n",
        "        emotion_label = get_emotion_vector(filename)\n",
        "        fasttext_embedding = load_fasttext_embedding(emotion_label, fasttext_folder)\n",
        "        fasttext_embedding = torch.tensor(fasttext_embedding, dtype=torch.float32)\n",
        "        fasttext_vector_dict[filename] = (wav2vec_embedding, fasttext_embedding)\n",
        "    return fasttext_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "iVgDqSIQ9xRg"
      },
      "outputs": [],
      "source": [
        "# filename_vector_dict = get_embeddings()\n",
        "# vector_map = map_fasttext_to_wav2vec(filename_vector_dict,fasttext_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC1_Hl0e_c1Z",
        "outputId": "02c90f55-9575-4f27-bbbd-c01016f3e43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_map loaded from pickle file.\n"
          ]
        }
      ],
      "source": [
        "def check_or_create_vector_map(pickle_path, fasttext_folder):\n",
        "    if os.path.exists(pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            if 'vector_map' in data:\n",
        "                print(\"vector_map loaded from pickle file.\")\n",
        "                return data['vector_map']\n",
        "\n",
        "    # If the file does not exist or vector_map is not in the file, create it\n",
        "    filename_vector_dict = get_embeddings()\n",
        "    vector_map = map_fasttext_to_wav2vec(filename_vector_dict, fasttext_folder)\n",
        "\n",
        "    # Save the vector_map to the pickle file\n",
        "    with open(pickle_path, 'wb') as f:\n",
        "        pickle.dump({'vector_map': vector_map}, f)\n",
        "    print(\"vector_map created and saved to pickle file.\")\n",
        "\n",
        "    return vector_map\n",
        "\n",
        "vector_map = check_or_create_vector_map(pickle_path, fasttext_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pShNR1_Dz0Ia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GZJVuNZrGTn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac72059-4c11-4f50-9a55-6569cacd4664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1152\n",
            "Test samples: 288\n",
            "Unseen emotion in training set: False\n",
            "Unseen emotion in test set: True\n",
            "Unseen emotion: angry\n"
          ]
        }
      ],
      "source": [
        "def split_sets(dictionary, unseen_emotion, train_ratio=0.8, seed=420):\n",
        "    # Set the random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Separate keys for the unseen emotion and other emotions\n",
        "    unseen_keys = [key for key in dictionary.keys() if get_emotion_vector(key) == unseen_emotion]\n",
        "    filtered_keys = [key for key in dictionary.keys() if key not in unseen_keys]\n",
        "\n",
        "    # Shuffle the filtered keys\n",
        "    random.shuffle(filtered_keys)\n",
        "\n",
        "    # Calculate the number of training samples needed from the filtered data\n",
        "    total_samples = len(dictionary)\n",
        "    num_train_samples = int(total_samples * train_ratio)\n",
        "    num_test_samples = total_samples - num_train_samples\n",
        "\n",
        "    # Adjust the number of test samples from the filtered data\n",
        "    num_test_samples_from_filtered = num_test_samples - len(unseen_keys)\n",
        "\n",
        "    # Ensure there are enough samples in the filtered data\n",
        "    if num_test_samples_from_filtered < 0:\n",
        "        raise ValueError(\"Not enough samples in the filtered data to maintain the overall split ratio.\")\n",
        "\n",
        "    # Split the filtered keys into training and test sets\n",
        "    train_keys = filtered_keys[:num_train_samples]\n",
        "    test_keys = filtered_keys[num_train_samples:num_train_samples + num_test_samples_from_filtered]\n",
        "\n",
        "    # Create training and test dictionaries from the filtered data\n",
        "    train_dict = {key: dictionary[key] for key in train_keys}\n",
        "    test_dict = {key: dictionary[key] for key in test_keys}\n",
        "\n",
        "    # Add the unseen emotion samples to the test dictionary\n",
        "    test_dict.update({key: dictionary[key] for key in unseen_keys})\n",
        "\n",
        "    # Check for overlaps\n",
        "    train_keys_set = set(train_dict.keys())\n",
        "    test_keys_set = set(test_dict.keys())\n",
        "    overlapping_keys = train_keys_set & test_keys_set\n",
        "    if overlapping_keys:\n",
        "        raise ValueError(f\"Overlapping filenames found between training and test sets: {overlapping_keys}\")\n",
        "\n",
        "\n",
        "    return train_dict, test_dict\n",
        "\n",
        "# Example usage\n",
        "train_dict, test_dict = split_sets(vector_map, unseen_emotion)\n",
        "\n",
        "# Check the counts\n",
        "print(\"Training samples:\", len(train_dict))\n",
        "print(\"Test samples:\", len(test_dict))\n",
        "\n",
        "# Ensure no unseen emotion samples in the training set\n",
        "print(\"Unseen emotion in training set:\", any(get_emotion_vector(key) == unseen_emotion for key in train_dict.keys()))\n",
        "print(\"Unseen emotion in test set:\", any(get_emotion_vector(key) == unseen_emotion for key in test_dict.keys()))\n",
        "print(\"Unseen emotion:\", unseen_emotion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9t1lGNK6-bq8"
      },
      "outputs": [],
      "source": [
        "class AdvancedEmbeddingMapper(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedEmbeddingMapper, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 128)\n",
        "        self.fc2 = nn.Linear(128, 300)\n",
        "        self.dropout = nn.Dropout(0.3)  # Increase dropout rate for better regularization\n",
        "        self.Tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.Tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "MBy2OlbuZ1q0"
      },
      "outputs": [],
      "source": [
        "class CNNEmbeddingMapper(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNNEmbeddingMapper, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm1d(256)\n",
        "    self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.bn3 = nn.BatchNorm1d(128)\n",
        "    self.conv4 = nn.Conv1d(in_channels=128, out_channels=300, kernel_size=3, padding=1)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x.transpose(1, 2)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNNEmbeddingMapper(nn.module):\n",
        "#   def __init__(self):"
      ],
      "metadata": {
        "id": "Iiuftd9VQ83m"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "40kfUBl--jN-"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(data_dict, batch_size=2, shuffle=True):\n",
        "    wav2vec_tensors = []\n",
        "    fasttext_tensors = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        wav2vec_tensors.append(data_dict[key][0])\n",
        "        fasttext_tensors.append(data_dict[key][1])\n",
        "\n",
        "    X = torch.stack(wav2vec_tensors)\n",
        "    Y = torch.stack(fasttext_tensors)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_dict)\n",
        "test_dataloader = create_dataloader(test_dict, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  model = define_model(trial).to(DEVICE)"
      ],
      "metadata": {
        "id": "w0RLOUpQDh4P"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ATCYpjY9-nZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8858e4a-51d2-40dd-c1c9-09eab0e6e099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.4961\n",
            "Epoch [2/5], Loss: 0.4658\n",
            "Epoch [3/5], Loss: 0.4586\n",
            "Epoch [4/5], Loss: 0.4475\n",
            "Epoch [5/5], Loss: 0.4399\n"
          ]
        }
      ],
      "source": [
        "model = AdvancedEmbeddingMapper()\n",
        "#CNNmodel = CNNEmbeddingMapper()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CosineEmbeddingLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, num_epochs=5):\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            # Create a label tensor filled with 1s\n",
        "            labels = torch.ones(outputs.size(0)).to(outputs.device)\n",
        "\n",
        "            #flatten\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            batch_y = batch_y.view(batch_y.size(0), -1)\n",
        "\n",
        "            loss = criterion(outputs, batch_y, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            # if counter >= 65:\n",
        "            #   torch.set_printoptions(profile='full')\n",
        "            #   for curr in batch_y:\n",
        "            #     print(curr[0])\n",
        "            # counter += 1\n",
        "            # print(counter)\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "            #evaluate_model_more(model, test_dataloader)\n",
        "\n",
        "\n",
        "train_model(model, train_dataloader)\n",
        "#train_model(CNNmodel, train_dataloader_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ajMf-OlnAeTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "9fb7492e-abd0-4988-ff96-83267c5a1578"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CosineEmbeddingLoss.forward() missing 1 required positional argument: 'target'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-3e4a8f6151aa>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'R-squared (R²): {r2:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mevaluate_model_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-3e4a8f6151aa>\u001b[0m in \u001b[0;36mevaluate_model_more\u001b[0;34m(model, test_dataloader, train_dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Squeeze to remove singleton dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CosineEmbeddingLoss.forward() missing 1 required positional argument: 'target'"
          ]
        }
      ],
      "source": [
        "def evaluate_model_more(model, test_dataloader, train_dataloader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_dataloader:\n",
        "            outputs = model(batch_x).squeeze(1)  # Squeeze to remove singleton dimension\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_targets.append(batch_y)\n",
        "            all_predictions.append(outputs)\n",
        "\n",
        "    # Compute average test loss\n",
        "    test_loss /= len(test_dataloader)\n",
        "    #train_loss /= len(train_dataloader)\n",
        "\n",
        "    # Concatenate all targets and predictions\n",
        "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
        "    all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
        "\n",
        "    # Compute additional metrics\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    #print(f'Train Loss: {train_loss:.4f}')\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "    print(f'R-squared (R²): {r2:.4f}')\n",
        "\n",
        "evaluate_model_more(model, test_dataloader, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8sq3esgLBIW"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(model_output, target):\n",
        "  cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "  output = cos(model_output, target)\n",
        "\n",
        "  return output\n",
        "\n",
        "# def evaluate_cosine():\n",
        "\n",
        "#   model = torch.tensor(model(batch_x).squeeze(1))\n",
        "#   target = torch.tensor(load_fasttext_embedding(unseen_emotion, fasttext_folder))\n",
        "\n",
        "# cosine_similarity(model, target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_cosine_similarity(tensor1, tensor2):\n",
        "    # Flatten the tensors if they are not 1-dimensional\n",
        "    if tensor1.dim() != 1:\n",
        "        tensor1 = tensor1.view(-1)\n",
        "    if tensor2.dim() != 1:\n",
        "        tensor2 = tensor2.view(-1)\n",
        "\n",
        "    # Compute the dot product between the two tensors\n",
        "    dot_product = torch.dot(tensor1, tensor2)\n",
        "\n",
        "    # Compute the L2 norm (Euclidean norm) of each tensor\n",
        "    norm_tensor1 = torch.norm(tensor1, p=2)\n",
        "    norm_tensor2 = torch.norm(tensor2, p=2)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_similarity = dot_product / (norm_tensor1 * norm_tensor2)\n",
        "\n",
        "    return cosine_similarity.item()"
      ],
      "metadata": {
        "id": "v4dgZ1Gs0RsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D2XbT0sZBgj"
      },
      "outputs": [],
      "source": [
        "# def evaluate_model_cosine(model, test_dataloader):\n",
        "#     model.eval()\n",
        "#     all_cosine_similarities = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch_x, batch_y in test_dataloader:\n",
        "#             outputs = model(batch_x)  # Pass batch_x to the model\n",
        "#             cosine_sim = custom_cosine_similarity(outputs, batch_y)\n",
        "#             all_cosine_similarities.extend(cosine_sim.cpu().numpy().tolist())  # Collect cosine similarities as scalar floats\n",
        "#     print\n",
        "#     # Compute average cosine similarity\n",
        "#     avg_cosine_similarity = np.mean(all_cosine_similarities)\n",
        "\n",
        "#     print(f'Average Cosine Similarity: {avg_cosine_similarity:.4f}')\n",
        "\n",
        "# # Example usage\n",
        "# # Assuming you have the model and dataloaders defined\n",
        "# evaluate_model_cosine(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_emotion(predicted_vector, emotion_vectors):\n",
        "    similarities = []\n",
        "    emotions = []\n",
        "\n",
        "    for emotion, vector in emotion_vectors.items():\n",
        "      similarity = custom_cosine_similarity(predicted_vector, vector)\n",
        "      similarities.append(similarity)\n",
        "      emotions.append(emotion)\n",
        "\n",
        "    # Convert similarities to numpy array and use argmax to find the highest similarity\n",
        "    similarities = np.array(similarities)\n",
        "    max_index = np.argmax(similarities)\n",
        "\n",
        "    most_similar_emotion = emotions[max_index]\n",
        "    max_similarity = similarities[max_index]\n",
        "\n",
        "    return most_similar_emotion, max_similarity"
      ],
      "metadata": {
        "id": "5LfcPchtWOvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors):\n",
        "#     model.eval()\n",
        "#     results = {}\n",
        "#     cosine_similarity_dict = {}\n",
        "#     correct_predictions = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         indices_list = list(test_dataloader.batch_sampler)\n",
        "\n",
        "#         for batch_idx, (batch_x, batch_y) in enumerate(test_dataloader):\n",
        "#             outputs = model(batch_x)  # Pass batch_x to the model\n",
        "#             batch_indices = indices_list[batch_idx]  # Get the corresponding indices for the current batch\n",
        "\n",
        "#             for i, output in enumerate(outputs):\n",
        "#                 global_index = batch_indices[i]\n",
        "#                 filename = list(test_dict.keys())[global_index]\n",
        "#                 cosine_sim = custom_cosine_similarity(output, batch_y[i])  # Calculate mean cosine similarity for the current sample\n",
        "#                 predicted_emotion, similarity_score = find_most_similar_emotion(output, emotion_vectors)\n",
        "#                 actual_emotion = get_emotion_vector(filename)\n",
        "\n",
        "#                 results[filename] = {\n",
        "#                     'cosine_similarity': cosine_sim,\n",
        "#                     'predicted_emotion': predicted_emotion,\n",
        "#                     'actual_emotion': actual_emotion\n",
        "#                 }\n",
        "#                 if predicted_emotion == actual_emotion:\n",
        "#                                 correct_predictions += 1\n",
        "#     print(f'Number of correct labels: {correct_predictions}')\n",
        "#     return results\n",
        "\n",
        "# emotion_vectors = load_emotion_vectors(fasttext_folder)\n",
        "# results = calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors)\n",
        "\n",
        "# # Print the results\n",
        "# for filename, result in results.items():\n",
        "#     print(f'{filename}: Cosine Similarity: {result[\"cosine_similarity\"]}, Predicted Emotion: {result[\"predicted_emotion\"]}, Actual Emotion: {result[\"actual_emotion\"]}')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wP4wAnt6CGDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors):\n",
        "  model.eval()\n",
        "  results = {}\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      indices_list = list(test_dataloader.batch_sampler)\n",
        "\n",
        "      for batch_idx, (batch_x, batch_y) in enumerate(test_dataloader):\n",
        "          outputs = model(batch_x)\n",
        "          batch_indices = indices_list[batch_idx]\n",
        "\n",
        "          for i, output in enumerate(outputs):\n",
        "              global_index = batch_indices[i]\n",
        "              filename = list(test_dict.keys())[global_index]\n",
        "              cosine_sim = custom_cosine_similarity(output, batch_y[i])\n",
        "              predicted_emotion, similarity_score = find_most_similar_emotion(output, emotion_vectors)\n",
        "              actual_emotion = get_emotion_vector(filename)\n",
        "              emotion_similarity = custom_cosine_similarity(output, emotion_vectors[predicted_emotion])\n",
        "\n",
        "              results[filename] = {\n",
        "                  'cosine_similarity': cosine_sim,\n",
        "                  'predicted_emotion': predicted_emotion,\n",
        "                  'actual_emotion': actual_emotion,\n",
        "                  'emotion_similarity': emotion_similarity\n",
        "              }\n",
        "              if predicted_emotion == actual_emotion:\n",
        "                  correct_predictions += 1\n",
        "\n",
        "  print(f'Number of correct labels: {correct_predictions}')\n",
        "  return results\n",
        "emotion_vectors = load_emotion_vectors(fasttext_folder)\n",
        "results = calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors)\n",
        "\n",
        "# Print the results\n",
        "for filename, result in results.items():\n",
        "    print(f'{filename}: Cosine Similarity: {result[\"cosine_similarity\"]}, Predicted Emotion: {result[\"predicted_emotion\"]}, '\n",
        "          f'Actual Emotion: {result[\"actual_emotion\"]}, Emotion Similarity: {result[\"emotion_similarity\"]}')"
      ],
      "metadata": {
        "id": "sb2wOKWiY8qM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HRjSWY4Fn7YN81YdrtYP5kaNk6gtnr6K",
      "authorship_tag": "ABX9TyNXjTJpyseSw2KYn7eWt9vY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}