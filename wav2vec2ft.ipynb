{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vZGkYqoM8tq7"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-guCYDAe9EBR"
      },
      "outputs": [],
      "source": [
        "#dataset\n",
        "unseen_emotion = \"sad\"\n",
        "folder = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "# where emotion word embeddings are stored\n",
        "fasttext_folder='/content/drive/MyDrive/emotion_vectors'\n",
        "#where to save the model\n",
        "model_save = 'advanced_embedding_mapper.pth'\n",
        "#where the pickle is saved\n",
        "pickle_path = '/content/drive/MyDrive/USER/fileTensorDict.pckl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "afP114cg84E9"
      },
      "outputs": [],
      "source": [
        "def get_emotion_vector(filename):\n",
        "    parts = filename.split('-')\n",
        "    third_number = parts[2]\n",
        "\n",
        "    emotion_vector_label = None\n",
        "    if third_number == '05':\n",
        "        emotion_vector_label = 'angry'\n",
        "    elif third_number == '02':\n",
        "        emotion_vector_label = 'calm'\n",
        "    elif third_number == '07':\n",
        "        emotion_vector_label = 'disgust'\n",
        "    elif third_number == '06':\n",
        "        emotion_vector_label = 'fearful'\n",
        "    elif third_number == '03':\n",
        "        emotion_vector_label = 'happy'\n",
        "    elif third_number == '01':\n",
        "        emotion_vector_label = 'neutral'\n",
        "    elif third_number == '04':\n",
        "        emotion_vector_label = 'sad'\n",
        "    elif third_number == '08':\n",
        "        emotion_vector_label = 'surprised'\n",
        "\n",
        "    return emotion_vector_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lsbj_J2r9Fg8"
      },
      "outputs": [],
      "source": [
        "def addToDict(folder):\n",
        "  emo_dict = {}\n",
        "  for file in os.listdir(folder):\n",
        "      emo_dict[file] = get_emotion_vector(file)\n",
        "  str_emo_dict = str(emo_dict)\n",
        "  with open(\"vectors.txt\", \"a\") as vec:\n",
        "    vec.write(str_emo_dict)\n",
        "\n",
        "for x in os.listdir(folder):\n",
        "  addToDict(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "39EHaFxC9P50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b70797-588f-41ff-c425-8c6d53ca0963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Aum2q6rN9X3m"
      },
      "outputs": [],
      "source": [
        "def get_vector_from_audio(path):\n",
        "  audio_input, sampling_rate = librosa.load(path, sr=16000)\n",
        "  inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "  hidden_states = outputs.last_hidden_state\n",
        "\n",
        "  vector_rep = torch.mean(hidden_states, dim=1)\n",
        "  return vector_rep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ydppCfjl9lgI"
      },
      "outputs": [],
      "source": [
        "def get_embeddings():\n",
        "  filename_vector_dict = {}\n",
        "  # path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "  for f in os.listdir(path):\n",
        "    file_path = os.path.join(path,f)\n",
        "    emov = get_vector_from_audio(file_path)\n",
        "    filename_vector_dict[f] = emov\n",
        "  return filename_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "CKsVEpCo9qIm"
      },
      "outputs": [],
      "source": [
        "def load_fasttext_embedding(emotion_label, fasttext_folder):\n",
        "  filepath = os.path.join(fasttext_folder, f'{emotion_label}.txt')\n",
        "  if not os.path.exists(filepath):\n",
        "      raise FileNotFoundError(f\"Embedding file for {emotion_label} not found in {folder}\")\n",
        "\n",
        "  embedding = []\n",
        "  with open(filepath, 'r') as file:\n",
        "      for line in file:\n",
        "          embedding.append(float(line.strip()))\n",
        "\n",
        "  return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ILxRzWeT9uqc"
      },
      "outputs": [],
      "source": [
        "def map_fasttext_to_wav2vec(wav2vec_dict, fasttext_folder='emotion_vectors'):\n",
        "    fasttext_vector_dict = {}\n",
        "    for filename, wav2vec_embedding in wav2vec_dict.items():\n",
        "        emotion_label = get_emotion_vector(filename)\n",
        "        fasttext_embedding = load_fasttext_embedding(emotion_label, fasttext_folder)\n",
        "        fasttext_embedding = torch.tensor(fasttext_embedding, dtype=torch.float32)\n",
        "        fasttext_vector_dict[filename] = (wav2vec_embedding, fasttext_embedding)\n",
        "    return fasttext_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "iVgDqSIQ9xRg"
      },
      "outputs": [],
      "source": [
        "# filename_vector_dict = get_embeddings()\n",
        "# vector_map = map_fasttext_to_wav2vec(filename_vector_dict,fasttext_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC1_Hl0e_c1Z",
        "outputId": "cb4b9a2d-b942-4bfc-e2ec-fe50a43df14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_map loaded from pickle file.\n"
          ]
        }
      ],
      "source": [
        "def check_or_create_vector_map(pickle_path, fasttext_folder):\n",
        "    if os.path.exists(pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            if 'vector_map' in data:\n",
        "                print(\"vector_map loaded from pickle file.\")\n",
        "                return data['vector_map']\n",
        "\n",
        "    # If the file does not exist or vector_map is not in the file, create it\n",
        "    filename_vector_dict = get_embeddings()\n",
        "    vector_map = map_fasttext_to_wav2vec(filename_vector_dict, fasttext_folder)\n",
        "\n",
        "    # Save the vector_map to the pickle file\n",
        "    with open(pickle_path, 'wb') as f:\n",
        "        pickle.dump({'vector_map': vector_map}, f)\n",
        "    print(\"vector_map created and saved to pickle file.\")\n",
        "\n",
        "    return vector_map\n",
        "\n",
        "vector_map = check_or_create_vector_map(pickle_path, fasttext_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZJVuNZrGTn9",
        "outputId": "c2e8f0d0-a297-4d62-e5fb-3d4792f75230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 48\n",
            "Test samples: 12\n",
            "Unseen emotion in training set: False\n",
            "Unseen emotion in test set: True\n",
            "Unseen emotion: sad\n"
          ]
        }
      ],
      "source": [
        "def split_sets(dictionary, unseen_emotion, train_ratio=0.8, seed=42):\n",
        "    # Set the random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Separate keys for the unseen emotion and other emotions\n",
        "    unseen_keys = [key for key in dictionary.keys() if get_emotion_vector(key) == unseen_emotion]\n",
        "    filtered_keys = [key for key in dictionary.keys() if key not in unseen_keys]\n",
        "\n",
        "    # Shuffle the filtered keys\n",
        "    random.shuffle(filtered_keys)\n",
        "\n",
        "    # Calculate the number of training samples needed from the filtered data\n",
        "    total_samples = len(dictionary)\n",
        "    num_train_samples = int(total_samples * train_ratio)\n",
        "    num_test_samples = total_samples - num_train_samples\n",
        "\n",
        "    # Adjust the number of test samples from the filtered data\n",
        "    num_test_samples_from_filtered = num_test_samples - len(unseen_keys)\n",
        "\n",
        "    # Ensure there are enough samples in the filtered data\n",
        "    if num_test_samples_from_filtered < 0:\n",
        "        raise ValueError(\"Not enough samples in the filtered data to maintain the overall split ratio.\")\n",
        "\n",
        "    # Split the filtered keys into training and test sets\n",
        "    train_keys = filtered_keys[:num_train_samples]\n",
        "    test_keys = filtered_keys[num_train_samples:num_train_samples + num_test_samples_from_filtered]\n",
        "\n",
        "    # Create training and test dictionaries from the filtered data\n",
        "    train_dict = {key: dictionary[key] for key in train_keys}\n",
        "    test_dict = {key: dictionary[key] for key in test_keys}\n",
        "\n",
        "    # Add the unseen emotion samples to the test dictionary\n",
        "    test_dict.update({key: dictionary[key] for key in unseen_keys})\n",
        "\n",
        "    # Check for overlaps\n",
        "    train_keys_set = set(train_dict.keys())\n",
        "    test_keys_set = set(test_dict.keys())\n",
        "    overlapping_keys = train_keys_set & test_keys_set\n",
        "    if overlapping_keys:\n",
        "        raise ValueError(f\"Overlapping filenames found between training and test sets: {overlapping_keys}\")\n",
        "\n",
        "\n",
        "    return train_dict, test_dict\n",
        "\n",
        "# Example usage\n",
        "train_dict, test_dict = split_sets(vector_map, unseen_emotion)\n",
        "\n",
        "# Check the counts\n",
        "print(\"Training samples:\", len(train_dict))\n",
        "print(\"Test samples:\", len(test_dict))\n",
        "\n",
        "# Ensure no unseen emotion samples in the training set\n",
        "print(\"Unseen emotion in training set:\", any(get_emotion_vector(key) == unseen_emotion for key in train_dict.keys()))\n",
        "print(\"Unseen emotion in test set:\", any(get_emotion_vector(key) == unseen_emotion for key in test_dict.keys()))\n",
        "print(\"Unseen emotion:\", unseen_emotion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9t1lGNK6-bq8"
      },
      "outputs": [],
      "source": [
        "class AdvancedEmbeddingMapper(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedEmbeddingMapper, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, 300)\n",
        "        self.dropout = nn.Dropout(0.3)  # Increase dropout rate for better regularization\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.bn1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.bn2(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.bn3(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "MBy2OlbuZ1q0"
      },
      "outputs": [],
      "source": [
        "class CNNEmbeddingMapper(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNNEmbeddingMapper, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm1d(256)\n",
        "    self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.bn3 = nn.BatchNorm1d(128)\n",
        "    self.conv4 = nn.Conv1d(in_channels=128, out_channels=300, kernel_size=3, padding=1)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x.transpose(1, 2)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "40kfUBl--jN-"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(data_dict, batch_size=32, shuffle=True):\n",
        "    wav2vec_tensors = []\n",
        "    fasttext_tensors = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        wav2vec_tensors.append(data_dict[key][0])\n",
        "        fasttext_tensors.append(data_dict[key][1])\n",
        "\n",
        "    X = torch.stack(wav2vec_tensors)\n",
        "    Y = torch.stack(fasttext_tensors)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_dict)\n",
        "test_dataloader = create_dataloader(test_dict, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_cnn(data_dict, batch_size=32, shuffle=True):\n",
        "    wav2vec_tensors = []\n",
        "    fasttext_tensors = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        wav2vec_tensors.append(data_dict[key][0])\n",
        "        fasttext_tensors.append(data_dict[key][1])\n",
        "\n",
        "    X = torch.stack(wav2vec_tensors)\n",
        "    Y = torch.stack(fasttext_tensors)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader_cnn = create_dataloader_cnn(train_dict)\n",
        "test_dataloader_cnn = create_dataloader_cnn(test_dict, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "qbEIi4HbGlsJ",
        "outputId": "580ba6a3-2374-400a-9687-64a20d474e99"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-60b89d433314>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_dataloader_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtest_dataloader_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloader_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-60b89d433314>\u001b[0m in \u001b[0;36mcreate_dataloader_cnn\u001b[0;34m(data_dict, batch_size, shuffle)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav2vec_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasttext_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "ATCYpjY9-nZQ",
        "outputId": "01d23e38-a9bc-413a-ff50-7c9f5a8637b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 300])) that is different to the input size (torch.Size([32, 1, 300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16, 300])) that is different to the input size (torch.Size([16, 1, 300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 0.1216\n",
            "Epoch [100/500], Loss: 0.1119\n",
            "Epoch [150/500], Loss: 0.1136\n",
            "Epoch [200/500], Loss: 0.1163\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-41bda3eea7fd>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#train_model(model, train_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-41bda3eea7fd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = AdvancedEmbeddingMapper()\n",
        "CNNmodel = CNNEmbeddingMapper()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, num_epochs=500):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "            #evaluate_model_more(model, test_dataloader)\n",
        "\n",
        "#train_model(model, train_dataloader)\n",
        "train_model(CNNmodel, train_dataloader_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228OT3KzDSVi"
      },
      "source": [
        "Why do I get **\"running_mean should contain 1 elements not 512\"** when BatchNorm1d(512) or 256 or 128?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMf-OlnAeTH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_more(model, test_dataloader):\n",
        "    CNNmodel.eval()\n",
        "    test_loss = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_dataloader:\n",
        "            outputs = model(batch_x).squeeze(1)  # Squeeze to remove singleton dimension\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_targets.append(batch_y)\n",
        "            all_predictions.append(outputs)\n",
        "\n",
        "    # Compute average test loss\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Concatenate all targets and predictions\n",
        "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
        "    all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
        "\n",
        "    # Compute additional metrics\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "    print(f'R-squared (R²): {r2:.4f}')\n",
        "\n",
        "evaluate_model_more(CNNmodel, test_dataloader_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8sq3esgLBIW"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(model, target):\n",
        "  cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "  output = cos(model, target)\n",
        "\n",
        "  return output\n",
        "\n",
        "# def evaluate_cosine():\n",
        "\n",
        "#   model = torch.tensor(model(batch_x).squeeze(1))\n",
        "#   target = torch.tensor(load_fasttext_embedding(unseen_emotion, fasttext_folder))\n",
        "\n",
        "# cosine_similarity(model, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D2XbT0sZBgj"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_cosine(model, test_dataloader):\n",
        "    model.eval()\n",
        "    all_cosine_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_dataloader:\n",
        "            outputs = model(batch_x)  # Pass batch_x to the model\n",
        "            cosine_sim = cosine_similarity(outputs, batch_y)\n",
        "            all_cosine_similarities.extend(cosine_sim.cpu().numpy().tolist())  # Collect cosine similarities as scalar floats\n",
        "\n",
        "    # Compute average cosine similarity\n",
        "    avg_cosine_similarity = np.mean(all_cosine_similarities)\n",
        "\n",
        "    print(f'Average Cosine Similarity: {avg_cosine_similarity:.4f}')\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have the model and dataloaders defined\n",
        "evaluate_model_cosine(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64zcRYBnMDgz"
      },
      "outputs": [],
      "source": [
        "input1 = torch.randn(99999999)\n",
        "input2 = torch.randn(99999999)\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "output = cos(input1, input2)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HRjSWY4Fn7YN81YdrtYP5kaNk6gtnr6K",
      "authorship_tag": "ABX9TyPsxelJPX/wHeJd74gm0Bvw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}