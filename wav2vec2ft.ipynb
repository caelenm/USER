{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HRjSWY4Fn7YN81YdrtYP5kaNk6gtnr6K",
      "authorship_tag": "ABX9TyMIpRcs2k1n7UBS/H+siS0O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vZGkYqoM8tq7"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "folder = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "# where emotion word embeddings are stored\n",
        "fasttext_folder='/content/drive/MyDrive/emotion_vectors'\n",
        "#where to save the model\n",
        "model_save = 'advanced_embedding_mapper.pth'\n",
        "#where the pickle is saved\n",
        "pickle_path = '/content/drive/MyDrive/USER/fileTensorDict.pckl'"
      ],
      "metadata": {
        "id": "-guCYDAe9EBR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emotion_vector(filename):\n",
        "    parts = filename.split('-')\n",
        "    third_number = parts[2]\n",
        "\n",
        "    emotion_vector_label = None\n",
        "    if third_number == '05':\n",
        "        emotion_vector_label = 'angry'\n",
        "    elif third_number == '02':\n",
        "        emotion_vector_label = 'calm'\n",
        "    elif third_number == '07':\n",
        "        emotion_vector_label = 'disgust'\n",
        "    elif third_number == '06':\n",
        "        emotion_vector_label = 'fearful'\n",
        "    elif third_number == '03':\n",
        "        emotion_vector_label = 'happy'\n",
        "    elif third_number == '01':\n",
        "        emotion_vector_label = 'neutral'\n",
        "    elif third_number == '04':\n",
        "        emotion_vector_label = 'sad'\n",
        "    elif third_number == '08':\n",
        "        emotion_vector_label = 'surprised'\n",
        "\n",
        "    return emotion_vector_label"
      ],
      "metadata": {
        "id": "afP114cg84E9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addToDict(folder):\n",
        "  emo_dict = {}\n",
        "  for file in os.listdir(folder):\n",
        "      emo_dict[file] = get_emotion_vector(file)\n",
        "  str_emo_dict = str(emo_dict)\n",
        "  with open(\"vectors.txt\", \"a\") as vec:\n",
        "    vec.write(str_emo_dict)\n",
        "\n",
        "for x in os.listdir(folder):\n",
        "  addToDict(folder)"
      ],
      "metadata": {
        "id": "lsbj_J2r9Fg8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39EHaFxC9P50",
        "outputId": "4c8ce0e6-ea30-4780-81f9-2c8a954162a9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_from_audio(path):\n",
        "  audio_input, sampling_rate = librosa.load(path, sr=16000)\n",
        "  inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "  hidden_states = outputs.last_hidden_state\n",
        "\n",
        "  vector_rep = torch.mean(hidden_states, dim=1)\n",
        "  return vector_rep\n",
        "\n"
      ],
      "metadata": {
        "id": "Aum2q6rN9X3m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings():\n",
        "  filename_vector_dict = {}\n",
        "  # path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "  for f in os.listdir(path):\n",
        "    file_path = os.path.join(path,f)\n",
        "    emov = get_vector_from_audio(file_path)\n",
        "    filename_vector_dict[f] = emov\n",
        "  return filename_vector_dict"
      ],
      "metadata": {
        "id": "ydppCfjl9lgI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fasttext_embedding(emotion_label, fasttext_folder):\n",
        "  filepath = os.path.join(fasttext_folder, f'{emotion_label}.txt')\n",
        "  if not os.path.exists(filepath):\n",
        "      raise FileNotFoundError(f\"Embedding file for {emotion_label} not found in {folder}\")\n",
        "\n",
        "  embedding = []\n",
        "  with open(filepath, 'r') as file:\n",
        "      for line in file:\n",
        "          embedding.append(float(line.strip()))\n",
        "\n",
        "  return embedding\n"
      ],
      "metadata": {
        "id": "CKsVEpCo9qIm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_fasttext_to_wav2vec(wav2vec_dict, fasttext_folder='emotion_vectors'):\n",
        "    fasttext_vector_dict = {}\n",
        "    for filename, wav2vec_embedding in wav2vec_dict.items():\n",
        "        emotion_label = get_emotion_vector(filename)\n",
        "        fasttext_embedding = load_fasttext_embedding(emotion_label, fasttext_folder)\n",
        "        fasttext_embedding = torch.tensor(fasttext_embedding, dtype=torch.float32)\n",
        "        fasttext_vector_dict[filename] = (wav2vec_embedding, fasttext_embedding)\n",
        "    return fasttext_vector_dict"
      ],
      "metadata": {
        "id": "ILxRzWeT9uqc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename_vector_dict = get_embeddings()\n",
        "# vector_map = map_fasttext_to_wav2vec(filename_vector_dict,fasttext_folder)"
      ],
      "metadata": {
        "id": "iVgDqSIQ9xRg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_or_create_vector_map(pickle_path, fasttext_folder):\n",
        "    if os.path.exists(pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            if 'vector_map' in data:\n",
        "                print(\"vector_map loaded from pickle file.\")\n",
        "                return data['vector_map']\n",
        "\n",
        "    # If the file does not exist or vector_map is not in the file, create it\n",
        "    filename_vector_dict = get_embeddings()\n",
        "    vector_map = map_fasttext_to_wav2vec(filename_vector_dict, fasttext_folder)\n",
        "\n",
        "    # Save the vector_map to the pickle file\n",
        "    with open(pickle_path, 'wb') as f:\n",
        "        pickle.dump({'vector_map': vector_map}, f)\n",
        "    print(\"vector_map created and saved to pickle file.\")\n",
        "\n",
        "    return vector_map\n",
        "\n",
        "vector_map = check_or_create_vector_map(pickle_path, fasttext_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC1_Hl0e_c1Z",
        "outputId": "a22e91dd-9e21-43c3-ff53-8f67fbb6dc5b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_map created and saved to pickle file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sets(dictionary, train_ratio=0.8, seed=42):\n",
        "    # Set the random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Get the list of keys and shuffle them\n",
        "    dict_keys = [key for key in dictionary.keys()]\n",
        "    random.shuffle(dict_keys)\n",
        "\n",
        "    # Determine the split index\n",
        "    split_index = int(len(dict_keys) * train_ratio)\n",
        "\n",
        "    # Split the keys into training and test sets\n",
        "    train_keys = dict_keys[:split_index]\n",
        "    test_keys = dict_keys[split_index:]\n",
        "\n",
        "    # Create training and test dictionaries\n",
        "    train_dict = {key: dictionary[key] for key in train_keys}\n",
        "    test_dict = {key: dictionary[key] for key in test_keys}\n",
        "\n",
        "    return train_dict, test_dict\n"
      ],
      "metadata": {
        "id": "iYcIBy3y97MG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSets(vector_map):\n",
        "  train_dict, test_dict = split_sets(vector_map)\n",
        "getSets(vector_map)"
      ],
      "metadata": {
        "id": "oLglh6I3-AHK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedEmbeddingMapper(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedEmbeddingMapper, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(1)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(1)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(1)\n",
        "        self.fc4 = nn.Linear(128, 300)\n",
        "        self.dropout = nn.Dropout(0.3)  # Increase dropout rate for better regularization\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9t1lGNK6-bq8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(data_dict, batch_size=32, shuffle=True):\n",
        "    wav2vec_tensors = []\n",
        "    fasttext_tensors = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        wav2vec_tensors.append(data_dict[key][0])\n",
        "        fasttext_tensors.append(data_dict[key][1])\n",
        "\n",
        "    X = torch.stack(wav2vec_tensors)\n",
        "    Y = torch.stack(fasttext_tensors)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_dict)\n",
        "test_dataloader = create_dataloader(test_dict, shuffle=False)"
      ],
      "metadata": {
        "id": "40kfUBl--jN-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AdvancedEmbeddingMapper()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, num_epochs=50):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "\n",
        "train_model(model, train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATCYpjY9-nZQ",
        "outputId": "6e6a9390-38a3-49de-f747-2e208687e038"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 300])) that is different to the input size (torch.Size([32, 1, 300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16, 300])) that is different to the input size (torch.Size([16, 1, 300])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Loss: 0.0615\n",
            "Epoch [10/50], Loss: 0.0385\n",
            "Epoch [15/50], Loss: 0.0287\n",
            "Epoch [20/50], Loss: 0.0242\n",
            "Epoch [25/50], Loss: 0.0200\n",
            "Epoch [30/50], Loss: 0.0182\n",
            "Epoch [35/50], Loss: 0.0164\n",
            "Epoch [40/50], Loss: 0.0147\n",
            "Epoch [45/50], Loss: 0.0132\n",
            "Epoch [50/50], Loss: 0.0127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do I get **\"running_mean should contain 1 elements not 512\"** when BatchNorm1d(512) or 256 or 128?"
      ],
      "metadata": {
        "id": "228OT3KzDSVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_more(model, test_dataloader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_dataloader:\n",
        "            outputs = model(batch_x).squeeze(1)  # Squeeze to remove singleton dimension\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_targets.append(batch_y)\n",
        "            all_predictions.append(outputs)\n",
        "\n",
        "    # Compute average test loss\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Concatenate all targets and predictions\n",
        "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
        "    all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
        "\n",
        "    # Compute additional metrics\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "    print(f'R-squared (R²): {r2:.4f}')\n",
        "\n",
        "evaluate_model_more(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajMf-OlnAeTH",
        "outputId": "8fd3c161-9a3b-401f-aa9f-90256b200ac9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0038\n",
            "Mean Squared Error (MSE): 0.0038\n",
            "Mean Absolute Error (MAE): 0.0470\n",
            "R-squared (R²): -0.4503\n"
          ]
        }
      ]
    }
  ]
}