{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "vZGkYqoM8tq7"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVDKt2Mhw-CT",
        "outputId": "2810ede5-6455-486b-81ba-a54c316ff2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.34.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.7.4)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install ray\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from ray import tune, air\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "from ray.air import session\n",
        "from ray.air.config import RunConfig, ScalingConfig\n",
        "from ray.tune import Tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-guCYDAe9EBR"
      },
      "outputs": [],
      "source": [
        "#dataset\n",
        "unseen_emotion = \"\"\n",
        "folder = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "# where emotion word embeddings are stored\n",
        "fasttext_folder='/content/drive/MyDrive/emotion_vectors'\n",
        "#where to save the model\n",
        "model_save = 'advanced_embedding_mapper.pth'\n",
        "#where the pickle is saved\n",
        "pickle_path = '/content/drive/MyDrive/USER/fileTensorDict.pckl'\n",
        "seed = 420"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "afP114cg84E9"
      },
      "outputs": [],
      "source": [
        "def get_emotion_vector(filename):\n",
        "    parts = filename.split('-')\n",
        "    third_number = parts[2]\n",
        "\n",
        "    emotion_vector_label = None\n",
        "    if third_number == '05':\n",
        "        emotion_vector_label = 'angry'\n",
        "    elif third_number == '02':\n",
        "        emotion_vector_label = 'calm'\n",
        "    elif third_number == '07':\n",
        "        emotion_vector_label = 'disgust'\n",
        "    elif third_number == '06':\n",
        "        emotion_vector_label = 'fearful'\n",
        "    elif third_number == '03':\n",
        "        emotion_vector_label = 'happy'\n",
        "    elif third_number == '01':\n",
        "        emotion_vector_label = 'neutral'\n",
        "    elif third_number == '04':\n",
        "        emotion_vector_label = 'sad'\n",
        "    elif third_number == '08':\n",
        "        emotion_vector_label = 'surprised'\n",
        "\n",
        "    return emotion_vector_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "keDBeSEvVfNo"
      },
      "outputs": [],
      "source": [
        "def load_emotion_vectors(folder):\n",
        "    emotion_vectors = {}\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            emotion_name = filename.split('.')[0]\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            with open(filepath, 'r') as file:\n",
        "                vector = [float(line.strip()) for line in file]\n",
        "                emotion_vectors[emotion_name] = torch.tensor(vector, dtype=torch.float32)\n",
        "    return emotion_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lsbj_J2r9Fg8"
      },
      "outputs": [],
      "source": [
        "def addToDict(folder):\n",
        "  emo_dict = {}\n",
        "  for file in os.listdir(folder):\n",
        "      emo_dict[file] = get_emotion_vector(file)\n",
        "  str_emo_dict = str(emo_dict)\n",
        "  with open(\"vectors.txt\", \"a\") as vec:\n",
        "    vec.write(str_emo_dict)\n",
        "\n",
        "for x in os.listdir(folder):\n",
        "  addToDict(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39EHaFxC9P50",
        "outputId": "94608502-8391-4c31-d3e8-e9b81bbabfea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aum2q6rN9X3m"
      },
      "outputs": [],
      "source": [
        "def get_vector_from_audio(path):\n",
        "  audio_input, sampling_rate = librosa.load(path, sr=16000)\n",
        "  inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "  with torch.no_grad():\n",
        "    outputs = wav_model(**inputs)\n",
        "\n",
        "  hidden_states = outputs.last_hidden_state\n",
        "\n",
        "  vector_rep = torch.mean(hidden_states, dim=1)\n",
        "  return vector_rep\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ydppCfjl9lgI"
      },
      "outputs": [],
      "source": [
        "def get_embeddings():\n",
        "  filename_vector_dict = {}\n",
        "  # path = r\"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01\"\n",
        "  for f in os.listdir(path):\n",
        "    file_path = os.path.join(path,f)\n",
        "    emov = get_vector_from_audio(file_path)\n",
        "    filename_vector_dict[f] = emov\n",
        "  return filename_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CKsVEpCo9qIm"
      },
      "outputs": [],
      "source": [
        "def load_fasttext_embedding(emotion_label, fasttext_folder):\n",
        "  filepath = os.path.join(fasttext_folder, f'{emotion_label}.txt')\n",
        "  if not os.path.exists(filepath):\n",
        "      raise FileNotFoundError(f\"Embedding file for {emotion_label} not found in {folder}\")\n",
        "\n",
        "  embedding = []\n",
        "  with open(filepath, 'r') as file:\n",
        "      for line in file:\n",
        "          embedding.append(float(line.strip()))\n",
        "\n",
        "  return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ILxRzWeT9uqc"
      },
      "outputs": [],
      "source": [
        "def map_fasttext_to_wav2vec(wav2vec_dict, fasttext_folder='emotion_vectors'):\n",
        "    fasttext_vector_dict = {}\n",
        "    for filename, wav2vec_embedding in wav2vec_dict.items():\n",
        "        emotion_label = get_emotion_vector(filename)\n",
        "        fasttext_embedding = load_fasttext_embedding(emotion_label, fasttext_folder)\n",
        "        fasttext_embedding = torch.tensor(fasttext_embedding, dtype=torch.float32)\n",
        "        fasttext_vector_dict[filename] = (wav2vec_embedding, fasttext_embedding)\n",
        "    return fasttext_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iVgDqSIQ9xRg"
      },
      "outputs": [],
      "source": [
        "# filename_vector_dict = get_embeddings()\n",
        "# vector_map = map_fasttext_to_wav2vec(filename_vector_dict,fasttext_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC1_Hl0e_c1Z",
        "outputId": "4c01caf7-2a01-4f75-af5a-0215ad21de17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_map loaded from pickle file.\n"
          ]
        }
      ],
      "source": [
        "def check_or_create_vector_map(pickle_path, fasttext_folder):\n",
        "    if os.path.exists(pickle_path):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            if 'vector_map' in data:\n",
        "                print(\"vector_map loaded from pickle file.\")\n",
        "                return data['vector_map']\n",
        "\n",
        "    # If the file does not exist or vector_map is not in the file, create it\n",
        "    filename_vector_dict = get_embeddings()\n",
        "    vector_map = map_fasttext_to_wav2vec(filename_vector_dict, fasttext_folder)\n",
        "\n",
        "    # Save the vector_map to the pickle file\n",
        "    with open(pickle_path, 'wb') as f:\n",
        "        pickle.dump({'vector_map': vector_map}, f)\n",
        "    print(\"vector_map created and saved to pickle file.\")\n",
        "\n",
        "    return vector_map\n",
        "\n",
        "vector_map = check_or_create_vector_map(pickle_path, fasttext_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pShNR1_Dz0Ia"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZJVuNZrGTn9",
        "outputId": "813cbd91-43b7-4074-ef03-d6ee228cef40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1152\n",
            "Test samples: 288\n",
            "Unseen emotion in training set: False\n",
            "Unseen emotion in test set: False\n",
            "Unseen emotion: \n"
          ]
        }
      ],
      "source": [
        "def split_sets(dictionary, unseen_emotion, train_ratio=0.8, seed=420):\n",
        "    # Set the random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Separate keys for the unseen emotion and other emotions\n",
        "    unseen_keys = [key for key in dictionary.keys() if get_emotion_vector(key) == unseen_emotion]\n",
        "    filtered_keys = [key for key in dictionary.keys() if key not in unseen_keys]\n",
        "\n",
        "    # Shuffle the filtered keys\n",
        "    random.shuffle(filtered_keys)\n",
        "\n",
        "    # Calculate the number of training samples needed from the filtered data\n",
        "    total_samples = len(dictionary)\n",
        "    num_train_samples = int(total_samples * train_ratio)\n",
        "    num_test_samples = total_samples - num_train_samples\n",
        "\n",
        "    # Adjust the number of test samples from the filtered data\n",
        "    num_test_samples_from_filtered = num_test_samples - len(unseen_keys)\n",
        "\n",
        "    # Ensure there are enough samples in the filtered data\n",
        "    if num_test_samples_from_filtered < 0:\n",
        "        raise ValueError(\"Not enough samples in the filtered data to maintain the overall split ratio.\")\n",
        "\n",
        "    # Split the filtered keys into training and test sets\n",
        "    train_keys = filtered_keys[:num_train_samples]\n",
        "    test_keys = filtered_keys[num_train_samples:num_train_samples + num_test_samples_from_filtered]\n",
        "\n",
        "    # Create training and test dictionaries from the filtered data\n",
        "    train_dict = {key: dictionary[key] for key in train_keys}\n",
        "    test_dict = {key: dictionary[key] for key in test_keys}\n",
        "\n",
        "    # Add the unseen emotion samples to the test dictionary\n",
        "    test_dict.update({key: dictionary[key] for key in unseen_keys})\n",
        "\n",
        "    # Check for overlaps\n",
        "    train_keys_set = set(train_dict.keys())\n",
        "    test_keys_set = set(test_dict.keys())\n",
        "    overlapping_keys = train_keys_set & test_keys_set\n",
        "    if overlapping_keys:\n",
        "        raise ValueError(f\"Overlapping filenames found between training and test sets: {overlapping_keys}\")\n",
        "\n",
        "\n",
        "    return train_dict, test_dict\n",
        "\n",
        "# Example usage\n",
        "train_dict, test_dict = split_sets(vector_map, unseen_emotion)\n",
        "\n",
        "# Check the counts\n",
        "print(\"Training samples:\", len(train_dict))\n",
        "print(\"Test samples:\", len(test_dict))\n",
        "\n",
        "# Ensure no unseen emotion samples in the training set\n",
        "print(\"Unseen emotion in training set:\", any(get_emotion_vector(key) == unseen_emotion for key in train_dict.keys()))\n",
        "print(\"Unseen emotion in test set:\", any(get_emotion_vector(key) == unseen_emotion for key in test_dict.keys()))\n",
        "print(\"Unseen emotion:\", unseen_emotion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MBy2OlbuZ1q0"
      },
      "outputs": [],
      "source": [
        "class CNNEmbeddingMapper(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNNEmbeddingMapper, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm1d(256)\n",
        "    self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.bn3 = nn.BatchNorm1d(128)\n",
        "    self.conv4 = nn.Conv1d(in_channels=128, out_channels=300, kernel_size=3, padding=1)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x.transpose(1, 2)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Iiuftd9VQ83m"
      },
      "outputs": [],
      "source": [
        "# class RNNEmbeddingMapper(nn.module):\n",
        "#   def __init__(self):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "40kfUBl--jN-"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(data_dict, batch_size=2, shuffle=True):\n",
        "    wav2vec_tensors = []\n",
        "    fasttext_tensors = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        wav2vec_tensors.append(data_dict[key][0])\n",
        "        fasttext_tensors.append(data_dict[key][1])\n",
        "\n",
        "    X = torch.stack(wav2vec_tensors)\n",
        "    Y = torch.stack(fasttext_tensors)\n",
        "\n",
        "    dataset = TensorDataset(X, Y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_dataloader = create_dataloader(train_dict)\n",
        "test_dataloader = create_dataloader(test_dict, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PU_04izByO5J"
      },
      "outputs": [],
      "source": [
        "def get_activation_function(name):\n",
        "    if name == 'ReLU':\n",
        "        return nn.ReLU()\n",
        "    elif name == 'Tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name == 'Sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif name == 'LeakyReLU':\n",
        "        return nn.LeakyReLU()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function: {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9t1lGNK6-bq8"
      },
      "outputs": [],
      "source": [
        "class AdvancedEmbeddingMapper(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout_rate, activation_function):\n",
        "        super(AdvancedEmbeddingMapper, self).__init__()\n",
        "        self.fc1 = nn.Linear(768, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 300)\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Increase dropout rate for better regularization\n",
        "        self.activation_function = get_activation_function(activation_function)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_function(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0RLOUpQDh4P",
        "outputId": "b89f58b9-617e-4b47-cdc6-cfee2e15fa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-13 19:01:07,090] A new study created in memory with name: no-name-9f5fab87-d18e-452e-bced-1e99ced3aa06\n",
            "<ipython-input-21-ab376fd02a47>:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "[W 2024-08-13 19:02:25,974] Trial 0 failed with parameters: {'optimizer': 'SGD', 'hidden_size': 653, 'dropout_rate': 0.35052041596716443, 'learning_rate': 0.030593109611005826, 'activation_function': 'Sigmoid', 'num_epochs': 58} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-21-ab376fd02a47>\", line 27, in objective\n",
            "    outputs = model(batch_x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-20-4e26cbdcc75f>\", line 13, in forward\n",
            "    x = self.dropout(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\", line 59, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1295, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
        "    hidden_size = trial.suggest_int('hidden_size', 128, 1024)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    activation_function = trial.suggest_categorical('activation_function', ['ReLU', 'Tanh', 'Sigmoid', 'LeakyReLU'])\n",
        "    num_epochs = trial.suggest_int('num_epochs', 5, 100)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AdvancedEmbeddingMapper(hidden_size, dropout_rate, activation_function).to(device)\n",
        "    criterion = nn.CosineEmbeddingLoss()\n",
        "\n",
        "    # Initialize optimizer\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs): #change 10 to num_epochs when training final model, takes too long\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            # Create a label tensor filled with 1s\n",
        "            labels = torch.ones(outputs.size(0)).to(outputs.device)\n",
        "\n",
        "            # Flatten\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            batch_y = batch_y.view(batch_y.size(0), -1)\n",
        "\n",
        "            loss = criterion(outputs, batch_y, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_dataloader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                outputs = outputs.view(outputs.size(0), -1)\n",
        "                batch_y = batch_y.view(batch_y.size(0), -1)\n",
        "\n",
        "                labels = torch.ones(outputs.size(0), device=device)\n",
        "                loss = criterion(outputs, batch_y, labels)\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "        valid_loss /= len(test_dataloader)\n",
        "        trial.report(valid_loss, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "# Create Optuna study and optimize\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=1)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print('Best hyperparameters:', study.best_params)\n",
        "best_params = study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATCYpjY9-nZQ"
      },
      "outputs": [],
      "source": [
        "best_params = study.best_params\n",
        "\n",
        "optimizer_name = best_params['optimizer']\n",
        "hidden_size = best_params['hidden_size']\n",
        "dropout_rate = best_params['dropout_rate']\n",
        "learning_rate = best_params['learning_rate']\n",
        "activation_function = best_params['activation_function']\n",
        "num_epochs = best_params['num_epochs']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CosineEmbeddingLoss()\n",
        "\n",
        "\n",
        "model = AdvancedEmbeddingMapper(hidden_size, dropout_rate, activation_function)\n",
        "#model = CNNEmbeddingMapper()\n",
        "\n",
        "# Initialize optimizer\n",
        "if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "elif optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "elif optimizer_name == 'RMSprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_model(model, train_dataloader, best_params):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            # Create a label tensor filled with 1s\n",
        "            labels = torch.ones(outputs.size(0)).to(outputs.device)\n",
        "\n",
        "            #flatten\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            batch_y = batch_y.view(batch_y.size(0), -1)\n",
        "\n",
        "            loss = criterion(outputs, batch_y, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_dataloader):.4f}')\n",
        "            #evaluate_model_more(model, test_dataloader)\n",
        "\n",
        "\n",
        "train_model(model, train_dataloader, best_params)\n",
        "\n",
        "\n",
        "# Save the model and optimizer state\n",
        "model_save_path = 'advanced_embedding_mapper.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'best_params': best_params\n",
        "}, model_save_path)\n",
        "\n",
        "print(f'Model saved to {model_save_path}')\n",
        "\n",
        "#train_model(CNNmodel, train_dataloader_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMf-OlnAeTH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_more(model, test_dataloader, train_dataloader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_dataloader:\n",
        "            outputs = model(batch_x).squeeze(1)  # Squeeze to remove singleton dimension\n",
        "\n",
        "            # Create a label tensor filled with 1s\n",
        "            labels = torch.ones(outputs.size(0)).to(outputs.device)\n",
        "\n",
        "            #flatten\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            batch_y = batch_y.view(batch_y.size(0), -1)\n",
        "\n",
        "            loss = criterion(outputs, batch_y, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_targets.append(batch_y)\n",
        "            all_predictions.append(outputs)\n",
        "\n",
        "    # Compute average test loss\n",
        "    test_loss /= len(test_dataloader)\n",
        "    #train_loss /= len(train_dataloader)\n",
        "\n",
        "    # Concatenate all targets and predictions\n",
        "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
        "    all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
        "\n",
        "    # Compute additional metrics\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    #print(f'Train Loss: {train_loss:.4f}')\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "    print(f'R-squared (R²): {r2:.4f}')\n",
        "\n",
        "evaluate_model_more(model, test_dataloader, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8sq3esgLBIW"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(model_output, target):\n",
        "  cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "  output = cos(model_output, target)\n",
        "\n",
        "  return output\n",
        "\n",
        "# def evaluate_cosine():\n",
        "\n",
        "#   model = torch.tensor(model(batch_x).squeeze(1))\n",
        "#   target = torch.tensor(load_fasttext_embedding(unseen_emotion, fasttext_folder))\n",
        "\n",
        "# cosine_similarity(model, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4dgZ1Gs0RsC"
      },
      "outputs": [],
      "source": [
        "def custom_cosine_similarity(tensor1, tensor2):\n",
        "    # Flatten the tensors if they are not 1-dimensional\n",
        "    if tensor1.dim() != 1:\n",
        "        tensor1 = tensor1.view(-1)\n",
        "    if tensor2.dim() != 1:\n",
        "        tensor2 = tensor2.view(-1)\n",
        "\n",
        "    # Compute the dot product between the two tensors\n",
        "    dot_product = torch.dot(tensor1, tensor2)\n",
        "\n",
        "    # Compute the L2 norm (Euclidean norm) of each tensor\n",
        "    norm_tensor1 = torch.norm(tensor1, p=2)\n",
        "    norm_tensor2 = torch.norm(tensor2, p=2)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_similarity = dot_product / (norm_tensor1 * norm_tensor2)\n",
        "\n",
        "    return cosine_similarity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LfcPchtWOvv"
      },
      "outputs": [],
      "source": [
        "def find_most_similar_emotion(predicted_vector, emotion_vectors):\n",
        "    similarities = []\n",
        "    emotions = []\n",
        "\n",
        "    for emotion, vector in emotion_vectors.items():\n",
        "      similarity = custom_cosine_similarity(predicted_vector, vector)\n",
        "      similarities.append(similarity)\n",
        "      emotions.append(emotion)\n",
        "\n",
        "    # Convert similarities to numpy array and use argmax to find the highest similarity\n",
        "    similarities = np.array(similarities)\n",
        "    max_index = np.argmax(similarities)\n",
        "\n",
        "    most_similar_emotion = emotions[max_index]\n",
        "    max_similarity = similarities[max_index]\n",
        "\n",
        "    return most_similar_emotion, max_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sb2wOKWiY8qM"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors):\n",
        "  model.eval()\n",
        "  results = {}\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      indices_list = list(test_dataloader.batch_sampler)\n",
        "\n",
        "      for batch_idx, (batch_x, batch_y) in enumerate(test_dataloader):\n",
        "          outputs = model(batch_x)\n",
        "          batch_indices = indices_list[batch_idx]\n",
        "\n",
        "          for i, output in enumerate(outputs):\n",
        "              global_index = batch_indices[i]\n",
        "              filename = list(test_dict.keys())[global_index]\n",
        "              cosine_sim = custom_cosine_similarity(output, batch_y[i])\n",
        "              predicted_emotion, similarity_score = find_most_similar_emotion(output, emotion_vectors)\n",
        "              actual_emotion = get_emotion_vector(filename)\n",
        "              emotion_similarity = custom_cosine_similarity(output, emotion_vectors[predicted_emotion])\n",
        "\n",
        "              results[filename] = {\n",
        "                  'cosine_similarity': cosine_sim,\n",
        "                  'predicted_emotion': predicted_emotion,\n",
        "                  'actual_emotion': actual_emotion,\n",
        "                  'emotion_similarity': emotion_similarity\n",
        "              }\n",
        "              if predicted_emotion == actual_emotion:\n",
        "                  correct_predictions += 1\n",
        "\n",
        "  print(f'Number of correct labels: {correct_predictions}')\n",
        "  return results\n",
        "emotion_vectors = load_emotion_vectors(fasttext_folder)\n",
        "results = calculate_cosine_similarity(model, test_dataloader, test_dict, emotion_vectors)\n",
        "\n",
        "# Print the results\n",
        "for filename, result in results.items():\n",
        "    print(f'{filename}: Cosine Similarity: {result[\"cosine_similarity\"]}, Predicted Emotion: {result[\"predicted_emotion\"]}, '\n",
        "          f'Actual Emotion: {result[\"actual_emotion\"]}, Emotion Similarity: {result[\"emotion_similarity\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujAGVFVR-fBO"
      },
      "source": [
        "**Testing ground below**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOMDKmip9thb"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "wav_file = \"/content/drive/MyDrive/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\"\n",
        "ft = fasttext.load_model('drive/MyDrive/fasttext/cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNRAtzHC_Z3T"
      },
      "outputs": [],
      "source": [
        "def knn(custom_vector):\n",
        "  # Get all words in the vocabulary\n",
        "  words = ft.get_words()\n",
        "\n",
        "  # Create a list to hold (word, similarity) pairs\n",
        "  similarity_list = []\n",
        "\n",
        "  for word in words:\n",
        "      word_vector = ft.get_word_vector(word)\n",
        "      similarity = cosine_similarity(custom_vector, word_vector)\n",
        "      similarity_list.append((word, similarity))\n",
        "\n",
        "  # Sort the list by similarity in descending order\n",
        "  similarity_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Get the top N similar words\n",
        "  top_n = 10  # Change this to get more or fewer words\n",
        "  top_similar_words = similarity_list[:top_n]\n",
        "\n",
        "  # Print the results\n",
        "  for word, similarity in top_similar_words:\n",
        "      print(f\"Word: {word}, Similarity: {similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg7S6VFD8vf1"
      },
      "outputs": [],
      "source": [
        "def sample_to_tensor(wav_file):\n",
        "    audio_input, sampling_rate = librosa.load(wav_file, sr=16000)  # Ensure the sample rate is 16kHz\n",
        "\n",
        "    # Process the audio to create the input tensor\n",
        "    inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Extract the input values (this is the tensor you'll pass to the model)\n",
        "    audio_tensor = inputs.input_values\n",
        "\n",
        "    return audio_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyCfRQ4F8bCv"
      },
      "outputs": [],
      "source": [
        "def run_model_on_sample(model, audio_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Pass through the model\n",
        "        audio_tensor = audio_tensor.unsqueeze(0)\n",
        "        output = model(audio_tensor)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C60prYU7-ojf"
      },
      "outputs": [],
      "source": [
        "tensor = get_vector_from_audio(wav_file)\n",
        "output = run_model_on_sample(model, tensor)\n",
        "knn(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HRjSWY4Fn7YN81YdrtYP5kaNk6gtnr6K",
      "authorship_tag": "ABX9TyPY3uYLmlFG1W4h6Epf/95A"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}